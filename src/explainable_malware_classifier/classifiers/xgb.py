from typing import Any

import numpy as np
import optuna
import shap
from matplotlib import pyplot as plt
from optuna import Trial, TrialPruned
from optuna.pruners import MedianPruner
from optuna.study import StudyDirection
from scipy.sparse import csc_matrix
from sklearn import clone
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, f1_score
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from xgboost.callback import EarlyStopping

from explainable_malware_classifier.classifiers.base import BaseMalwareClassifier
from explainable_malware_classifier.classifiers.utils import save_plot
from explainable_malware_classifier.utils import timer


# Cross-Validation f1_macro scores: [0.792, 0.7751, 0.8048, 0.7861, 0.7904, 0.7889, 0.7777, 0.796, 0.79, 0.753]
# Cross-Validation f1_macro mean:   0.7854
# Cross-Validation f1_macro std:    0.0142
# --------------------------------------------------
# Classification Report:
#                precision    recall  f1-score   support
#
#       adware       0.83      0.72      0.77       282
#     backdoor       0.84      0.80      0.82       352
#   downloader       0.82      0.81      0.82       231
#      dropper       0.65      0.69      0.67       173
#      spyware       0.63      0.62      0.63       172
#       trojan       0.93      0.96      0.95      2883
#        virus       0.95      0.96      0.95      1076
#        worms       0.77      0.65      0.70       399
#
#     accuracy                           0.89      5568
#    macro avg       0.80      0.78      0.79      5568
# weighted avg       0.89      0.89      0.89      5568
#
# --------------------------------------------------
# Finished MalwareClassifierXGB.run() in 1653.05 secs


class MalwareClassifierXGB(BaseMalwareClassifier):
    model_name = "xgb.joblib"

    def __init__(
        self,
        max_features: int = 10_000,
        ngram_range: tuple[int, int] = (1, 2),
        patience: int | None = 50,
        max_depth: int = 10,
        learning_rate: float = 0.1837,
        n_estimators: int = 400,
        subsample: float = 0.7692,
        colsample_bytree: float = 0.6295,
        min_child_weight: float = 3,
        gamma: float = 0.00003,
        tree_method="hist",
        random_state=69,
        verbosity=2,
        device="cuda",
        n_jobs=None,
    ):
        self.random_state = random_state
        self.patience = patience
        self.vectorizer = TfidfVectorizer(
            tokenizer=self.comma_tokenizer,
            token_pattern=None,
            max_features=max_features,
            ngram_range=ngram_range,
            sublinear_tf=False,
        )
        self.label_encoder = LabelEncoder()
        callbacks = []
        if patience:
            callbacks.append(
                EarlyStopping(
                    rounds=self.patience,
                    save_best=True,
                    maximize=True,
                    metric_name="f1_macro",
                )
            )
        self.classifier = XGBClassifier(
            objective="multi:softmax",
            eval_metric=self.f1_macro,
            callbacks=callbacks,
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            min_child_weight=min_child_weight,
            gamma=gamma,
            booster="gbtree",
            tree_method=tree_method,
            random_state=self.random_state,
            verbosity=verbosity,
            n_jobs=n_jobs,
            device=device,
            disable_default_eval_metric=True,
        )

    @staticmethod
    def f1_macro(y_pred: np.ndarray, y_true: np.ndarray):
        score = f1_score(y_true, y_pred, average="macro")
        return score

    def tune_hyperparameters(
        self, X: csc_matrix, y: np.ndarray, *, n_trials: int | None
    ):
        # TODO: needs more tuning
        # Best hyperparameters: {'max_depth': 10, 'learning_rate': 0.1836571052713742, 'n_estimators': 400, 'subsample': 0.7692127413124975, 'colsample_bytree': 0.6294841326490792, 'min_child_weight': 3, 'gamma': 3.743302045781916e-05}
        # Best hyperparameters: {'max_depth': 10, 'min_child_weight': 0.0006939313862091159}
        def objective(trial: Trial):
            params = {
                "max_depth": trial.suggest_int("max_depth", 1, 20),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                "n_estimators": trial.suggest_categorical(
                    "n_estimators", [100, 200, 300, 400, 500]
                ),
                "subsample": trial.suggest_float("subsample", 0.7, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                "min_child_weight": trial.suggest_categorical(
                    "min_child_weight", [1, 3, 5, 7]
                ),
                "gamma": trial.suggest_float("gamma", 0, 0.01),
            }
            kfold = StratifiedKFold(
                n_splits=4, shuffle=True, random_state=self.random_state
            )
            scores = []
            for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, y)):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                model: XGBClassifier = clone(self.classifier)
                model.set_params(**params)
                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
                y_pred = model.predict(X_val)
                score = f1_score(y_val, y_pred, average="macro")
                scores.append(score)

                trial.report(np.mean(scores), step=fold_idx)
                if trial.should_prune():
                    raise TrialPruned()
            return np.mean(scores)

        study = optuna.create_study(
            direction=StudyDirection.MAXIMIZE,
            pruner=MedianPruner(n_startup_trials=4, n_warmup_steps=1),
        )
        study.optimize(
            objective, n_trials=n_trials, gc_after_trial=False, show_progress_bar=True
        )
        print("Best hyperparameters:", study.best_trial.params)
        self.classifier.set_params(**study.best_trial.params)

    def cross_validate(
        self,
        X: csc_matrix,
        y: np.ndarray,
        *,
        cv_splits: int,
        scoring: str = "f1_macro",
    ):
        kfold = StratifiedKFold(
            n_splits=cv_splits, shuffle=True, random_state=self.random_state
        )
        scores = []
        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, y), 1):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            fold_model: XGBClassifier = clone(self.classifier)
            fold_model.fit(X_train, y_train, eval_set=[(X_val, y_val)])
            y_pred = fold_model.predict(X_val)
            fold_score = f1_score(y_val, y_pred, average=scoring.split("_")[1])
            scores.append(float(fold_score))
            print(f"Fold {fold_idx}, {scoring}={fold_score:.4f}")

        self.display_cv_results(scoring, scores)

    def train_and_evaluate(
        self, X: np.ndarray, y: np.ndarray, *, test_size: float = 0.2
    ) -> None:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )
        self.classifier.fit(X_train, y_train, eval_set=[(X_test, y_test)])
        y_pred_encoded = self.classifier.predict(X_test)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)
        y_true = self.label_encoder.inverse_transform(y_test)
        self.plot_confusion_matrix(
            y_true, y_pred, title="XGB Confusion Matrix", save_as="xgb_conf_matrix"
        )
        print("Classification Report:\n", classification_report(y_true, y_pred))
        print("-" * 50)
        self.save_model_artifacts(X_train, X_test, y_train, y_test)

    def get_model_artifacts(self) -> dict[str, Any]:
        artifacts = super().get_model_artifacts()
        artifacts["model"] = self.classifier
        return artifacts

    def explain(self):
        artifacts = self.load_model_artifacts()
        model = artifacts["model"]
        feature_names = artifacts["feature_names"]
        X_test, y_test = artifacts["X_test"], artifacts["y_test"]
        explainer = shap.TreeExplainer(model, feature_names=feature_names)
        explanation = explainer(X_test)
        y_pred = model.predict(X_test)
        max_display = 30
        plt.figure(clear=True)
        for class_idx, class_name in enumerate(artifacts["label_encoder"].classes_):
            class_explanation = explanation[:, :, class_idx]
            plt.clf()
            shap.plots.beeswarm(
                class_explanation,
                max_display=max_display,
                group_remaining_features=False,
                show=False,
            )
            save_plot(
                f"SHAP Beeswarm Plot for Class: {class_name} (Mean Avg)",
                f"xgb/shap/beeswarm/mean_avg/{class_name}",
            )
            max_positive_shap = np.max(
                np.where(class_explanation.values > 0, class_explanation.values, 0),
                axis=0,
            )
            order = np.argsort(-max_positive_shap)
            plt.clf()
            shap.plots.beeswarm(
                class_explanation,
                max_display=max_display,
                order=order,
                group_remaining_features=False,
                show=False,
            )
            save_plot(
                f"SHAP Beeswarm Plot for Class: {class_name} (High Positive)",
                f"xgb/shap/beeswarm/high_positive/{class_name}",
            )
            true_indices = np.where((y_test == class_idx) & (y_pred == class_idx))[0]
            false_indices = np.where((y_test == class_idx) & (y_pred != class_idx))[0]
            if true_indices.size > 0:
                idx = true_indices[0]
                instance_explanation = class_explanation[idx]
                instance_features = X_test[idx]
                plt.clf()
                shap.plots.decision(
                    instance_explanation.base_values,
                    instance_explanation.values,
                    instance_features,
                    feature_names,
                    show=False,
                )
                save_plot(
                    f"SHAP Decision Plot for Class: {class_name} (Correct)",
                    f"xgb/shap/decision/correct/{class_name}",
                )
            if false_indices.size > 0:
                idx = false_indices[0]
                instance_explanation = class_explanation[idx]
                instance_features = X_test[idx]
                plt.clf()
                shap.plots.decision(
                    instance_explanation.base_values,
                    instance_explanation.values,
                    instance_features,
                    feature_names,
                    show=False,
                )
                save_plot(
                    f"SHAP Decision Plot for Class: {class_name} (Misclassified)",
                    f"xgb/shap/decision/misclassified/{class_name}",
                )
        plt.close("all")

    @timer
    def run(self) -> None:
        X, y = self.load_and_transform_data()
        # self.tune_hyperparameters(X, y, n_trials=20)
        self.cross_validate(X, y, cv_splits=10)
        self.train_and_evaluate(X.toarray(), y)
        self.explain()
