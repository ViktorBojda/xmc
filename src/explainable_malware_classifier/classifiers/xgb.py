from typing import Any

import numpy as np
import optuna
from optuna import Trial, TrialPruned
from optuna.pruners import MedianPruner
from optuna.study import StudyDirection
from scipy.sparse import csc_matrix
from sklearn import clone
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, f1_score
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
import joblib
from xgboost import XGBClassifier
from xgboost.callback import EarlyStopping

from explainable_malware_classifier.classifiers.base import BaseMalwareClassifier
from explainable_malware_classifier.classifiers.utils import (
    comma_tokenizer,
    load_dataset,
    timer,
)

# Cross-Validation f1_macro scores: [0.8003, 0.7667, 0.8018, 0.7776, 0.7833, 0.7831, 0.7812, 0.7969, 0.7844, 0.7594]
# Cross-Validation f1_macro mean:   0.7835
# Cross-Validation f1_macro std:    0.0138
# --------------------------------------------------
# Classification Report:
#                precision    recall  f1-score   support
#
#       adware       0.84      0.71      0.77       282
#     backdoor       0.85      0.80      0.82       352
#   downloader       0.82      0.81      0.81       231
#      dropper       0.63      0.70      0.66       173
#      spyware       0.64      0.61      0.63       172
#       trojan       0.93      0.96      0.95      2883
#        virus       0.95      0.96      0.95      1076
#        worms       0.77      0.65      0.70       399
#
#     accuracy                           0.89      5568
#    macro avg       0.80      0.77      0.79      5568
# weighted avg       0.89      0.89      0.89      5568
#
# --------------------------------------------------
# Model artifacts have been saved to: xgb_model.pkl
# Finished MalwareClassifierXGB.run() in 1197.29 secs


class MalwareClassifierXGB(BaseMalwareClassifier):
    def __init__(
        self,
        max_features: int = 10_000,
        ngram_range: tuple[int, int] = (1, 2),
        patience: int | None = 50,
        max_depth: int = 10,
        learning_rate: float = 0.1837,
        n_estimators: int = 400,
        subsample: float = 0.7692,
        colsample_bytree: float = 0.6295,
        min_child_weight: float = 3,
        gamma: float = 0.00003,
        tree_method="hist",
        random_state=69,
        verbosity=2,
        device="cuda",
        n_jobs=None,
    ):
        self.random_state = random_state
        self.patience = patience
        self.vectorizer = TfidfVectorizer(
            tokenizer=comma_tokenizer,
            token_pattern=None,
            max_features=max_features,
            ngram_range=ngram_range,
            sublinear_tf=False,
        )
        self.label_encoder = LabelEncoder()
        callbacks = []
        if patience:
            callbacks.append(
                EarlyStopping(
                    rounds=self.patience,
                    save_best=True,
                    maximize=True,
                    metric_name="f1_macro",
                )
            )
        self.classifier = XGBClassifier(
            objective="multi:softmax",
            eval_metric=self.f1_macro,
            callbacks=callbacks,
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            min_child_weight=min_child_weight,
            gamma=gamma,
            tree_method=tree_method,
            random_state=self.random_state,
            verbosity=verbosity,
            n_jobs=n_jobs,
            device=device,
            disable_default_eval_metric=True,
        )

    @staticmethod
    def f1_macro(y_pred: np.ndarray, y_true: np.ndarray):
        score = f1_score(y_true, y_pred, average="macro")
        return score

    def load_and_transform_data(
        self, dataset_name: str
    ) -> tuple[csc_matrix, np.ndarray]:
        df = load_dataset(dataset_name)
        X = csc_matrix(self.vectorizer.fit_transform(df["api"]))
        y_encoded = self.label_encoder.fit_transform(df["class"])
        return X, y_encoded

    def tune_hyperparameters(
        self, X: csc_matrix, y: np.ndarray, *, n_trials: int | None
    ):
        # TODO: needs more tuning
        # Best hyperparameters: {'max_depth': 10, 'learning_rate': 0.1836571052713742, 'n_estimators': 400, 'subsample': 0.7692127413124975, 'colsample_bytree': 0.6294841326490792, 'min_child_weight': 3, 'gamma': 3.743302045781916e-05}
        # Best hyperparameters: {'max_depth': 10, 'min_child_weight': 0.0006939313862091159}
        def objective(trial: Trial):
            params = {
                "max_depth": trial.suggest_int("max_depth", 1, 20),
                "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
                "n_estimators": trial.suggest_categorical(
                    "n_estimators", [100, 200, 300, 400, 500]
                ),
                "subsample": trial.suggest_float("subsample", 0.7, 1.0),
                "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                "min_child_weight": trial.suggest_categorical(
                    "min_child_weight", [1, 3, 5, 7]
                ),
                "gamma": trial.suggest_float("gamma", 0, 0.01),
            }
            kfold = StratifiedKFold(
                n_splits=4, shuffle=True, random_state=self.random_state
            )
            scores = []
            for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, y)):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                model: XGBClassifier = clone(self.classifier)
                model.set_params(**params)
                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
                y_pred = model.predict(X_val)
                score = f1_score(y_val, y_pred, average="macro")
                scores.append(score)

                trial.report(np.mean(scores), step=fold_idx)
                if trial.should_prune():
                    raise TrialPruned()
            return np.mean(scores)

        study = optuna.create_study(
            direction=StudyDirection.MAXIMIZE,
            pruner=MedianPruner(n_startup_trials=4, n_warmup_steps=1),
        )
        study.optimize(
            objective, n_trials=n_trials, gc_after_trial=False, show_progress_bar=True
        )
        print("Best hyperparameters:", study.best_trial.params)
        self.classifier.set_params(**study.best_trial.params)

    def cross_validate(
        self,
        X: csc_matrix,
        y: np.ndarray,
        *,
        cv_splits: int,
        scoring: str = "f1_macro",
    ):
        kfold = StratifiedKFold(
            n_splits=cv_splits, shuffle=True, random_state=self.random_state
        )
        scores = []
        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, y), 1):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            fold_model: XGBClassifier = clone(self.classifier)
            fold_model.fit(X_train, y_train, eval_set=[(X_val, y_val)])
            y_pred = fold_model.predict(X_val)
            fold_score = f1_score(y_val, y_pred, average=scoring.split("_")[1])
            scores.append(float(fold_score))
            print(f"Fold {fold_idx}, {scoring}={fold_score:.4f}")

        self.display_cv_results(scoring, scores)

    def train_and_evaluate(
        self, X: csc_matrix, y: np.ndarray, test_size: float = 0.2
    ) -> None:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )
        self.classifier.fit(X_train, y_train, eval_set=[(X_test, y_test)])
        y_pred_encoded = self.classifier.predict(X_test)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)
        y_true = self.label_encoder.inverse_transform(y_test)
        self.plot_confusion_matrix(
            y_true, y_pred, title="XGB Confusion Matrix", save_as="xgb_conf_matrix.png"
        )
        print("Classification Report:\n", classification_report(y_true, y_pred))
        print("-" * 50)

    def save_model(self, filename: str) -> None:
        joblib.dump((self.classifier, self.vectorizer, self.label_encoder), filename)
        print(f"Model artifacts have been saved to: {filename}")

    @staticmethod
    def load_model(filename: str) -> tuple[Any, Any, Any]:
        classifier, vectorizer, label_encoder = joblib.load(filename)
        return classifier, vectorizer, label_encoder

    @timer
    def run(self) -> None:
        X, y = self.load_and_transform_data("preprocessed_merged_seq.csv")
        # self.tune_hyperparameters(X, y, n_trials=20)
        self.cross_validate(X, y, cv_splits=10)
        self.train_and_evaluate(X, y)
        self.save_model("xgb_model.pkl")
