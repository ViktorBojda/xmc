import os
from copy import deepcopy
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import Optimizer
from torch.utils.data import DataLoader, TensorDataset
from timeit import default_timer as timer

from scipy.sparse import csc_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import classification_report, f1_score
from sklearn.preprocessing import LabelEncoder
import joblib

from explainable_malware_classifier.classifiers.base import BaseMalwareClassifier
from explainable_malware_classifier.classifiers.utils import (
    comma_tokenizer,
    load_dataset,
)


# Cross-Validation f1_macro scores: [0.7897, 0.7727, 0.8012, 0.7807, 0.7788, 0.7805, 0.7783, 0.7929, 0.7755, 0.7669]
# Cross-Validation f1_macro mean:   0.7817
# Cross-Validation f1_macro std:    0.0102
# --------------------------------------------------
# Classification Report:
#                precision    recall  f1-score   support
#
#       adware       0.77      0.73      0.75       282
#     backdoor       0.82      0.77      0.79       352
#   downloader       0.75      0.77      0.76       231
#      dropper       0.64      0.69      0.66       173
#      spyware       0.56      0.63      0.59       172
#       trojan       0.94      0.95      0.94      2883
#        virus       0.96      0.95      0.95      1076
#        worms       0.74      0.66      0.70       399
#
#     accuracy                           0.88      5568
#    macro avg       0.77      0.77      0.77      5568
# weighted avg       0.88      0.88      0.88      5568
#
# --------------------------------------------------
# Model artifacts have been saved to: mlp_model.joblib
# Total Runtime(s): 1225.09


class MalwareClassifierMLP(BaseMalwareClassifier):
    class MalwareNet(nn.Module):
        def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.LeakyReLU(negative_slope=0.01),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.LeakyReLU(negative_slope=0.01),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim, num_classes),
            )

        def forward(self, x):
            return self.net(x)

    def __init__(
        self,
        max_features: int = 10_000,
        ngram_range: tuple[int, int] = (1, 2),
        epochs: int = 200,
        patience: int | None = 30,
        batch_size: int = 256,
        hidden_dim: int = 256,
        learning_rate: float = 0.001,
        device: str = None,
        num_workers: int = -1,
        random_state: int = 69,
    ):
        self.random_state = random_state
        self.device = (
            device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        )
        self.num_workers = num_workers if num_workers != -1 else os.cpu_count()
        self.epochs = epochs
        self.patience = patience
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.hidden_dim = hidden_dim
        self.vectorizer = TfidfVectorizer(
            tokenizer=comma_tokenizer,
            token_pattern=None,
            max_features=max_features,
            ngram_range=ngram_range,
            sublinear_tf=True,
        )
        self.label_encoder = LabelEncoder()
        # set after fit_transform
        self.model = None
        self.input_dim = None
        self.num_classes = None
        self.set_random_seed()

    def set_random_seed(self):
        import random

        torch.manual_seed(self.random_state)
        if self.device == "cuda":
            torch.cuda.manual_seed_all(self.random_state)
        np.random.seed(self.random_state)
        random.seed(self.random_state)

    def load_and_transform_data(
        self, dataset_name: str
    ) -> tuple[np.ndarray, np.ndarray]:
        df = load_dataset(dataset_name)
        X_csr = self.vectorizer.fit_transform(df["api"])
        X = csc_matrix(X_csr).toarray()  # convert to dense for PyTorch
        y_encoded = self.label_encoder.fit_transform(df["class"])
        self.input_dim = X.shape[1]
        self.num_classes = len(self.label_encoder.classes_)
        self.model = self.MalwareNet(self.input_dim, self.hidden_dim, self.num_classes)
        self.model.to(self.device)
        return X, y_encoded

    def _prepare_train_test_data(
        self,
        model: MalwareNet,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_test: np.ndarray,
        y_test: np.ndarray,
    ) -> tuple[DataLoader, DataLoader, nn.Module, Optimizer]:
        # convert train and test folds to DataLoaders for batching, shuffle and performance
        train_ds = TensorDataset(
            torch.tensor(X_train, dtype=torch.float32),
            torch.tensor(y_train, dtype=torch.long),
        )
        test_ds = TensorDataset(
            torch.tensor(X_test, dtype=torch.float32),
            torch.tensor(y_test, dtype=torch.long),
        )
        train_loader = DataLoader(
            train_ds,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
        )
        test_loader = DataLoader(
            test_ds,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
        )
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(
            model.parameters(), lr=self.learning_rate, weight_decay=0.01
        )
        return train_loader, test_loader, criterion, optimizer

    def _train_one_epoch(
        self,
        model: MalwareNet,
        train_loader: DataLoader,
        criterion: Any,
        optimizer: Any,
    ) -> float:
        model.train()
        train_loss = 0.0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
            optimizer.zero_grad()
            outputs = model(batch_x.float())
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * batch_x.size(0)
        train_loss /= len(train_loader.dataset)
        return train_loss

    def _evaluate(
        self, model: MalwareNet, test_loader: DataLoader
    ) -> tuple[list, list]:
        model.eval()
        true_labels, pred_labels = [], []
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)
                outputs = model(batch_x.float())
                pred = torch.argmax(outputs, dim=1).cpu().numpy()
                pred_labels.extend(pred)
                true_labels.extend(batch_y.numpy())
        return true_labels, pred_labels

    def _train(
        self,
        model: MalwareNet,
        train_loader: DataLoader,
        test_loader: DataLoader,
        criterion: nn.Module,
        optimizer: Optimizer,
    ):
        best_f1 = 0.0
        patience_counter = 0
        best_model_state = None
        for epoch in range(self.epochs):
            train_loss = self._train_one_epoch(
                model, train_loader, criterion, optimizer
            )
            val_true, val_pred = self._evaluate(model, test_loader)
            curr_f1 = f1_score(val_true, val_pred, average="macro")
            print(
                f"Epoch {epoch + 1}/{self.epochs}, Train Loss: {train_loss:.4f}, Val Macro F1: {curr_f1:.4f}"
            )
            if self.patience is None:
                continue
            if curr_f1 > best_f1:
                best_f1 = curr_f1
                best_model_state = deepcopy(model.state_dict())
                patience_counter = 0
            else:
                patience_counter += 1
                print(f"No improvement for {patience_counter} epoch(s).")
                if patience_counter >= self.patience:
                    print("Early stopping triggered.")
                    break
        if best_model_state:
            model.load_state_dict(best_model_state)
        print("Training complete.")

    def cross_validate(
        self,
        X: np.ndarray,
        y: np.ndarray,
        cv_splits: int = 10,
        scoring: str = "f1_macro",
    ):
        """
        Performs cross-validation in a manner similar to scikit-learn.
        """
        kfold = StratifiedKFold(
            n_splits=cv_splits, shuffle=True, random_state=self.random_state
        )
        scores = []
        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, y), 1):
            # re-init model for each fold
            fold_model = self.MalwareNet(
                self.input_dim, self.hidden_dim, self.num_classes
            ).to(self.device)
            train_loader, val_loader, criterion, optimizer = (
                self._prepare_train_test_data(
                    fold_model, X[train_idx], y[train_idx], X[val_idx], y[val_idx]
                )
            )
            self._train(fold_model, train_loader, val_loader, criterion, optimizer)
            val_true, val_pred = self._evaluate(fold_model, val_loader)
            fold_score = f1_score(val_true, val_pred, average=scoring.split("_")[1])
            scores.append(float(fold_score))
            print(f"Fold {fold_idx}, {scoring}={fold_score:.4f}")

        print(
            f"Cross-Validation {scoring} scores:", [round(score, 4) for score in scores]
        )
        print(f"Cross-Validation {scoring} mean:   {np.mean(scores):.4f}")
        print(f"Cross-Validation {scoring} std:    {np.std(scores, ddof=1):.4f}")
        print("-" * 50)

    def train_and_evaluate(self, X: np.ndarray, y: np.ndarray, test_size: float = 0.2):
        """
        Train on the training split, then evaluate on the test split.
        """
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )
        train_loader, test_loader, criterion, optimizer = self._prepare_train_test_data(
            self.model, X_train, y_train, X_test, y_test
        )
        self._train(self.model, train_loader, test_loader, criterion, optimizer)
        y_true_encoded, y_pred_encoded = self._evaluate(self.model, test_loader)
        y_true = self.label_encoder.inverse_transform(y_true_encoded)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)
        self.plot_confusion_matrix(
            y_true,
            y_pred,
            title="MLP Confusion Matrix",
            save_as="mlp_conf_matrix.png",
        )
        print("Classification Report:\n", classification_report(y_true, y_pred))
        print("-" * 50)

    def save_model(self, filename: str):
        """
        Save the PyTorch model state_dict, along with vectorizer and label_encoder
        in a joblib file for convenience.
        """
        model_artifacts = {
            "model_state_dict": self.model.state_dict(),
            "input_dim": next(self.model.parameters()).shape[1],
            "hidden_dim": self.hidden_dim,
            "label_encoder": self.label_encoder,
            "vectorizer": self.vectorizer,
        }
        joblib.dump(model_artifacts, filename)
        print(f"Model artifacts have been saved to: {filename}")

    @staticmethod
    def load_model(filename: str):
        """
        Load the model artifacts, restore the model, vectorizer, and label encoder.
        Returns a tuple: (restored_model, vectorizer, label_encoder)
        """
        model_artifacts = joblib.load(filename)
        input_dim = model_artifacts["input_dim"]
        hidden_dim = model_artifacts["hidden_dim"]
        label_encoder = model_artifacts["label_encoder"]
        vectorizer = model_artifacts["vectorizer"]

        # reconstruct the model architecture
        num_classes = len(label_encoder.classes_)
        restored_model = MalwareClassifierMLP.MalwareNet(
            input_dim, hidden_dim, num_classes
        )
        restored_model.load_state_dict(model_artifacts["model_state_dict"])
        restored_model.eval()

        return restored_model, vectorizer, label_encoder

    def run(self) -> None:
        start_time = timer()
        X, y = self.load_and_transform_data("preprocessed_merged_seq.csv")
        self.cross_validate(X, y, cv_splits=10, scoring="f1_macro")
        self.train_and_evaluate(X, y)
        self.save_model("mlp_model.joblib")
        print("Total Runtime(s):", round(timer() - start_time, 2))
