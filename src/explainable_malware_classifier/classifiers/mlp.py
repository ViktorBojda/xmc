import os
from copy import deepcopy
from typing import Any

import numpy as np
import shap
import torch
import torch.nn as nn
import torch.optim as optim
from matplotlib import pyplot as plt
from torch.optim import Optimizer
from torch.utils.data import DataLoader, TensorDataset

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import classification_report, f1_score
from sklearn.preprocessing import LabelEncoder

from explainable_malware_classifier.classifiers import save_plot
from explainable_malware_classifier.classifiers.base import BaseMalwareClassifier
from explainable_malware_classifier.utils import timer


# Cross-Validation f1_macro scores: [0.7897, 0.7727, 0.8012, 0.7807, 0.7788, 0.7805, 0.7783, 0.7929, 0.7755, 0.7669]
# Cross-Validation f1_macro mean:   0.7817
# Cross-Validation f1_macro std:    0.0102
# --------------------------------------------------
# Classification Report:
#                precision    recall  f1-score   support
#
#       adware       0.77      0.73      0.75       282
#     backdoor       0.82      0.77      0.79       352
#   downloader       0.75      0.77      0.76       231
#      dropper       0.64      0.69      0.66       173
#      spyware       0.56      0.63      0.59       172
#       trojan       0.94      0.95      0.94      2883
#        virus       0.96      0.95      0.95      1076
#        worms       0.74      0.66      0.70       399
#
#     accuracy                           0.88      5568
#    macro avg       0.77      0.77      0.77      5568
# weighted avg       0.88      0.88      0.88      5568
#
# --------------------------------------------------
# Model artifacts have been saved to: mlp_model.joblib
# Total Runtime(s): 1225.09


class MalwareClassifierMLP(BaseMalwareClassifier):
    model_name = "mlp.joblib"

    class MalwareNet(nn.Module):
        def __init__(self, input_dim: int, hidden_dim: int, num_classes: int):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.LeakyReLU(negative_slope=0.01),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.LeakyReLU(negative_slope=0.01),
                nn.Dropout(0.3),
                nn.Linear(hidden_dim, num_classes),
            )

        def forward(self, x):
            return self.net(x)

    def __init__(
        self,
        max_features: int = 10_000,
        ngram_range: tuple[int, int] = (1, 2),
        epochs: int = 200,
        patience: int | None = 30,
        batch_size: int = 256,
        hidden_dim: int = 256,
        learning_rate: float = 0.001,
        device: str = None,
        num_workers: int = -1,
        random_state: int = 69,
    ):
        self.random_state = random_state
        self.device = (
            device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        )
        self.num_workers = num_workers if num_workers != -1 else os.cpu_count()
        self.epochs = epochs
        self.patience = patience
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.hidden_dim = hidden_dim
        self.vectorizer = TfidfVectorizer(
            tokenizer=self.comma_tokenizer,
            token_pattern=None,
            max_features=max_features,
            ngram_range=ngram_range,
            sublinear_tf=True,
        )
        self.label_encoder = LabelEncoder()
        # set after fit_transform
        self.model = None
        self.input_dim = None
        self.num_classes = None
        self.set_random_seed()

    def set_random_seed(self):
        import random

        torch.manual_seed(self.random_state)
        if self.device == "cuda":
            torch.cuda.manual_seed_all(self.random_state)
        np.random.seed(self.random_state)
        random.seed(self.random_state)

    def load_and_transform_data(self) -> tuple[np.ndarray, np.ndarray]:
        X, y = super().load_and_transform_data()
        X = X.toarray()  # convert to dense for PyTorch
        self.input_dim = X.shape[1]
        self.num_classes = len(self.label_encoder.classes_)
        self.model = self.MalwareNet(self.input_dim, self.hidden_dim, self.num_classes)
        self.model.to(self.device)
        return X, y

    def _prepare_train_test_data(
        self,
        model: MalwareNet,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_test: np.ndarray,
        y_test: np.ndarray,
    ) -> tuple[DataLoader, DataLoader, nn.Module, Optimizer]:
        # convert train and test folds to DataLoaders for batching, shuffle and performance
        train_ds = TensorDataset(
            torch.tensor(X_train, dtype=torch.float32),
            torch.tensor(y_train, dtype=torch.long),
        )
        test_ds = TensorDataset(
            torch.tensor(X_test, dtype=torch.float32),
            torch.tensor(y_test, dtype=torch.long),
        )
        train_loader = DataLoader(
            train_ds,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=True,
        )
        test_loader = DataLoader(
            test_ds,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True,
        )
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(
            model.parameters(), lr=self.learning_rate, weight_decay=0.01
        )
        return train_loader, test_loader, criterion, optimizer

    def _train_one_epoch(
        self,
        model: MalwareNet,
        train_loader: DataLoader,
        criterion: Any,
        optimizer: Any,
    ) -> float:
        model.train()
        train_loss = 0.0
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
            optimizer.zero_grad()
            outputs = model(batch_x.float())
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * batch_x.size(0)
        train_loss /= len(train_loader.dataset)
        return train_loss

    def _evaluate(
        self, model: MalwareNet, test_loader: DataLoader
    ) -> tuple[list, list]:
        model.eval()
        true_labels, pred_labels = [], []
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)
                outputs = model(batch_x.float())
                batch_preds = torch.argmax(outputs, dim=1).cpu().numpy()
                pred_labels.extend(batch_preds)
                true_labels.extend(batch_y.numpy())
        return true_labels, pred_labels

    def _train(
        self,
        model: MalwareNet,
        train_loader: DataLoader,
        test_loader: DataLoader,
        criterion: nn.Module,
        optimizer: Optimizer,
    ) -> None:
        best_f1 = 0.0
        patience_counter = 0
        best_model_state = None
        for epoch in range(self.epochs):
            train_loss = self._train_one_epoch(
                model, train_loader, criterion, optimizer
            )
            val_true, val_pred = self._evaluate(model, test_loader)
            curr_f1 = f1_score(val_true, val_pred, average="macro")
            print(
                f"Epoch {epoch + 1}/{self.epochs}, Train Loss: {train_loss:.4f}, Val Macro F1: {curr_f1:.4f}"
            )
            if self.patience is None:
                continue
            if curr_f1 > best_f1:
                best_f1 = curr_f1
                best_model_state = deepcopy(model.state_dict())
                patience_counter = 0
            else:
                patience_counter += 1
                print(f"No improvement for {patience_counter} epoch(s).")
                if patience_counter >= self.patience:
                    print("Early stopping triggered.")
                    break
        if best_model_state:
            model.load_state_dict(best_model_state)
        print("Training complete.")

    def cross_validate(
        self,
        X: np.ndarray,
        y: np.ndarray,
        *,
        cv_splits: int = 10,
        scoring: str = "f1_macro",
    ) -> None:
        """
        Performs cross-validation in a manner similar to scikit-learn.
        """
        kfold = StratifiedKFold(
            n_splits=cv_splits, shuffle=True, random_state=self.random_state
        )
        scores = []
        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, y), 1):
            # re-init model for each fold
            fold_model = self.MalwareNet(
                self.input_dim, self.hidden_dim, self.num_classes
            ).to(self.device)
            train_loader, val_loader, criterion, optimizer = (
                self._prepare_train_test_data(
                    fold_model, X[train_idx], y[train_idx], X[val_idx], y[val_idx]
                )
            )
            self._train(fold_model, train_loader, val_loader, criterion, optimizer)
            val_true, val_pred = self._evaluate(fold_model, val_loader)
            fold_score = f1_score(val_true, val_pred, average=scoring.split("_")[1])
            scores.append(float(fold_score))
            print(f"Fold {fold_idx}, {scoring}={fold_score:.4f}")

        self.display_cv_results(scoring, scores)

    def train_and_evaluate(
        self, X: np.ndarray, y: np.ndarray, *, test_size: float = 0.2
    ) -> None:
        """
        Train on the training split, then evaluate on the test split.
        """
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )
        train_loader, test_loader, criterion, optimizer = self._prepare_train_test_data(
            self.model, X_train, y_train, X_test, y_test
        )
        self._train(self.model, train_loader, test_loader, criterion, optimizer)
        y_true_encoded, y_pred_encoded = self._evaluate(self.model, test_loader)
        y_true = self.label_encoder.inverse_transform(y_true_encoded)
        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)
        self.plot_confusion_matrix(
            y_true, y_pred, title="MLP Confusion Matrix", save_as="mlp_conf_matrix"
        )
        print("Classification Report:\n", classification_report(y_true, y_pred))
        print("-" * 50)
        self.save_model_artifacts(X_train, X_test, y_train, y_test)

    def get_model_artifacts(self) -> dict[str, Any]:
        artifacts = super().get_model_artifacts()
        artifacts.update(
            {
                "model_state_dict": self.model.state_dict(),
                "input_dim": next(self.model.parameters()).shape[1],
                "hidden_dim": self.hidden_dim,
            }
        )
        return artifacts

    @classmethod
    def load_model_artifacts(cls) -> dict[str, Any]:
        artifacts = super().load_model_artifacts()
        # reconstruct the model
        num_classes = len(artifacts["label_encoder"].classes_)
        model = MalwareClassifierMLP.MalwareNet(
            artifacts["input_dim"], artifacts["hidden_dim"], num_classes
        )
        model.load_state_dict(artifacts["model_state_dict"])
        model.eval()
        artifacts["model"] = model
        return artifacts

    def explain(self):
        # Finished MalwareClassifierMLP.run() in 3555.68 secs
        artifacts = self.load_model_artifacts()
        model = artifacts["model"]
        feature_names = artifacts["feature_names"]
        label_encoder = artifacts["label_encoder"]
        X_train = artifacts["X_train"]
        X_test, y_test = artifacts["X_test"], artifacts["y_test"]

        X_train_tensor = torch.tensor(X_train[:500], dtype=torch.float32)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
        explainer = shap.DeepExplainer(model, X_train_tensor)
        shap_values = explainer.shap_values(X_test_tensor)
        base_values = explainer.expected_value
        with torch.no_grad():
            outputs = model(X_test_tensor)
            y_pred = torch.argmax(outputs, dim=1).cpu().numpy()
        max_display = 30
        plt.figure(clear=True)
        for class_idx, class_name in enumerate(label_encoder.classes_):
            class_explanation = shap.Explanation(
                base_values=base_values[class_idx],
                values=shap_values[:, :, class_idx],
                data=X_test_tensor,
                feature_names=feature_names,
            )
            plt.clf()
            shap.plots.beeswarm(
                class_explanation,
                max_display=max_display,
                group_remaining_features=False,
                show=False,
            )
            save_plot(
                f"SHAP Beeswarm Plot for Class: {class_name} (Mean Avg)",
                f"mlp/shap/beeswarm/mean_avg/{class_name}",
            )
            max_positive_shap = np.max(
                np.where(class_explanation.values > 0, class_explanation.values, 0),
                axis=0,
            )
            order = np.argsort(-max_positive_shap)
            plt.clf()
            shap.plots.beeswarm(
                class_explanation,
                max_display=max_display,
                order=order,
                group_remaining_features=False,
                show=False,
            )
            save_plot(
                f"SHAP Beeswarm Plot for Class: {class_name} (High Positive)",
                f"mlp/shap/beeswarm/high_positive/{class_name}",
            )
            true_indices = np.where((y_test == class_idx) & (y_pred == class_idx))[0]
            false_indices = np.where((y_test == class_idx) & (y_pred != class_idx))[0]
            if true_indices.size > 0:
                idx = true_indices[0]
                instance_explanation = class_explanation[idx]
                instance_features = X_test[idx]
                plt.clf()
                shap.plots.decision(
                    instance_explanation.base_values,
                    instance_explanation.values,
                    instance_features,
                    feature_names,
                    show=False,
                )
                save_plot(
                    f"SHAP Decision Plot for Class: {class_name} (Correct)",
                    f"mlp/shap/decision/correct/{class_name}",
                )
            if false_indices.size > 0:
                idx = false_indices[0]
                instance_explanation = class_explanation[idx]
                instance_features = X_test[idx]
                plt.clf()
                shap.plots.decision(
                    instance_explanation.base_values,
                    instance_explanation.values,
                    instance_features,
                    feature_names,
                    show=False,
                )
                save_plot(
                    f"SHAP Decision Plot for Class: {class_name} (Misclassified)",
                    f"mlp/shap/decision/misclassified/{class_name}",
                )
        plt.close("all")

    @timer
    def run(self) -> None:
        X, y = self.load_and_transform_data()
        self.cross_validate(X, y, cv_splits=10, scoring="f1_macro")
        self.train_and_evaluate(X, y)
        self.explain()
