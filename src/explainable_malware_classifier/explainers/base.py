from __future__ import annotations
import datetime as dt
import random
import traceback
from abc import ABC, abstractmethod
from collections.abc import Callable
from enum import StrEnum
from typing import TYPE_CHECKING

import numpy as np
import shap
from alibi.explainers import AnchorTabular
from matplotlib import pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import compute_class_weight

from explainable_malware_classifier.classifiers.utils import save_plot
from explainable_malware_classifier.settings import EXPLANATIONS_DIR_PATH
from explainable_malware_classifier.utils import prompt_options

if TYPE_CHECKING:
    from explainable_malware_classifier.classifiers.base import BaseMalwareClassifier


class BaseMalwareExplainer(ABC):
    def __init__(
        self, classifier_class: type[BaseMalwareClassifier], random_state: int = 69
    ) -> None:
        self.classifier_class = classifier_class
        self.random_state = random_state
        self.run()

    @abstractmethod
    def explain_shap(self) -> None: ...

    @abstractmethod
    def explain_anchors(self) -> None: ...

    def run(self) -> None:
        class ExplanationMethod(StrEnum):
            SHAP = "SHAP"
            ANCHORS = "Anchors"

        explanation_methods = {
            ExplanationMethod.SHAP: self.explain_shap,
            ExplanationMethod.ANCHORS: self.explain_anchors,
        }
        print("Choose which explanation method to run, options are:")
        explanation_method, explanation_name = prompt_options(explanation_methods)
        print(f"Running {explanation_name} explanation...")
        explanation_method()

    def plot_shap_explanations(
        self,
        explanation_getter: Callable[[int], shap.Explanation],
        X_test: np.ndarray,
        y_test: np.ndarray,
        y_pred: np.ndarray,
        feature_names: list[str],
        label_encoder: LabelEncoder,
        beeswarm_max_display: int = 30,
    ):
        plt.figure(clear=True)
        for class_idx, class_name in enumerate(label_encoder.classes_):
            class_explanation = explanation_getter(class_idx)
            plt.clf()
            shap.plots.beeswarm(
                class_explanation,
                max_display=beeswarm_max_display,
                group_remaining_features=False,
                show=False,
            )
            save_plot(
                f"SHAP Beeswarm Plot for Class: {class_name} (Mean Avg)",
                f"{self.classifier_class.model_name}/beeswarm/mean_avg/{class_name}",
            )
            max_positive_shap = np.max(
                np.where(class_explanation.values > 0, class_explanation.values, 0),
                axis=0,
            )
            order = np.argsort(-max_positive_shap)
            plt.clf()
            shap.plots.beeswarm(
                class_explanation,
                max_display=beeswarm_max_display,
                order=order,
                group_remaining_features=False,
                show=False,
            )
            save_plot(
                f"SHAP Beeswarm Plot for Class: {class_name} (Max Positive)",
                f"{self.classifier_class.model_name}/beeswarm/max_positive/{class_name}",
            )

            def create_decision_plot(instance_idxs: np.ndarray, identifier: str):
                if instance_idxs.size == 0:
                    return print(
                        f"Failed to create decision plot, no correct prediction found for class {class_name}."
                    )
                instance_idx = instance_idxs[0]
                instance_explanation = class_explanation[instance_idx]
                instance_features = X_test[instance_idx]
                plt.clf()
                shap.plots.decision(
                    instance_explanation.base_values,
                    instance_explanation.values,
                    instance_features,
                    feature_names,
                    show=False,
                )
                save_plot(
                    f"SHAP Decision Plot for Class: {class_name} ({identifier.capitalize()})",
                    f"{self.classifier_class.model_name}/decision/{identifier.lower()}/{class_name}",
                )

            correct_idxs = np.where((y_test == class_idx) & (y_pred == class_idx))[0]
            create_decision_plot(correct_idxs, "correct")
            incorrect_idxs = np.where((y_test == class_idx) & (y_pred != class_idx))[0]
            create_decision_plot(incorrect_idxs, "misclassified")

            print(f"SHAP plots created for class {class_name}.")
        plt.close("all")

    def create_anchors_explanations(
        self,
        predictor: Callable[[np.ndarray], np.ndarray],
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_test: np.ndarray,
        y_test: np.ndarray,
        feature_names: list[str],
        label_encoder: LabelEncoder,
    ):
        explainer = AnchorTabular(
            predictor=predictor, feature_names=feature_names, seed=self.random_state
        )
        explainer.fit(X_train)
        y_pred = predictor(X_test)
        class_weights = compute_class_weight(
            class_weight="balanced", classes=np.unique(y_train), y=y_train
        )
        SAMPLES_PER_CLASS = 5
        thresholds = {"strict": 0.025, "general": 0.025}
        base_dir = EXPLANATIONS_DIR_PATH / f"{self.classifier_class.model_name}/anchors"
        random.seed(self.random_state)
        for class_idx, class_name in enumerate(label_encoder.classes_):
            correct_idxs = np.where((y_test == class_idx) & (y_pred == class_idx))[0]
            if not correct_idxs.size:
                print(
                    f"Failed to create anchors, no correct prediction found for class '{class_name}'."
                )
                continue

            sampled_idxs = random.sample(
                correct_idxs.tolist(), k=min(SAMPLES_PER_CLASS, len(correct_idxs))
            )
            if (sample_length := len(sampled_idxs)) != SAMPLES_PER_CLASS:
                print(
                    f"Failed to find {SAMPLES_PER_CLASS} correct predictions for class '{class_name}', "
                    f"generating anchors for the {sample_length} samples found."
                )

            for idx in sampled_idxs:
                instance = X_test[idx]
                instance_dir = base_dir / f"{class_name}/instance_{idx}"
                instance_dir.mkdir(parents=True, exist_ok=True)

                for mode, threshold in thresholds.items():
                    instance_descriptor = (
                        f"instance of class '{class_name}', index {idx}, mode '{mode}'"
                    )
                    try:
                        explanation = explainer.explain(
                            instance,
                            threshold=threshold,
                            beam_size=4,
                            verbose=True,
                            tau=0.98,
                        )
                        (instance_dir / f"explanation_{mode}.json").write_text(
                            explanation.to_json()
                        )
                        if explanation.anchor:
                            rules = "\nAND ".join(explanation.anchor)
                            result = (
                                f"Anchors explanation for {instance_descriptor}:\n"
                                f"Precision: {round(explanation.precision, 4)}\n"
                                f"Coverage: {explanation.coverage}\n"
                                f"Weighted coverage: {round(class_weights[class_idx] * explanation.coverage, 4)}\n"
                                f"Anchor:\nIF {rules}\nTHEN PREDICT {class_name}\n"
                            )
                        else:
                            result = f"No anchor found for {instance_descriptor}.\n"
                        print(result)
                        (instance_dir / f"explanation_{mode}.txt").write_text(result)
                    except Exception as e:
                        timestamp = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        error_msg = (
                            f"[{timestamp}] Error while generating anchor for {instance_descriptor}:\n"
                            f"{str(e)}\n"
                        )
                        print(error_msg)
                        with (base_dir / f"error_logs.txt").open("a") as f:
                            f.write(error_msg)
                            f.write(f"Traceback:\n{traceback.format_exc()}\n")
                            f.write("-" * 50 + "\n")
                    print("-" * 50)
            print(f"Anchors created for class {class_name}.")
